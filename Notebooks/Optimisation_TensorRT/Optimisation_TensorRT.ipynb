{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a0ce6-202c-46cf-ba24-651a3535da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30cd6c-4969-4195-99f9-df4167dc9585",
   "metadata": {},
   "source": [
    "# Conversion du modèle vers un modèle TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f7879-f1fc-405f-93fc-8845b065627c",
   "metadata": {},
   "source": [
    "TensorRT réalise plusieurs modifications et optimisations sur le réseau. Tout d'abord, les couches n'utilisant pas de sorties sont supprimées afin d'alléger les calculs. Ensuite, lorsque cela est possible, les couches de convolution, les offsets (bias) et les activations sont fusionnées pour ne former qu'une couche unique.  \n",
    "Vous pouvez trouver des informations relatives à TensorRT sur le site de Nvidia : https://developer.nvidia.com/tensorrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd04c2a-d1a6-4188-892c-258313ef6f36",
   "metadata": {},
   "source": [
    "Pour réaliser la converison du modèle, nous utilisons l'outil trtexec, en précisant le répertoire dans lequel le modèle est sauvegardé et les paramètres de conversion :  \n",
    "- **precision_mode** : Format de codage des nombres : FP32, FP16 ou INT8. Les formats inférieurs au FP32 (FP16 et INT8) peuvent améliorer les performances des calculs. Le mode FP16 utilise des coeurs matériels avec des instructions sur des flottants 16bits lorsque cela est possible. Le mode INT8 utilise des coeurs matériels avec des instructions sur des entiers.     \n",
    "- **max_batch_size** : Le batch-size maximum à utiliser pendant l'optimisation. Pendant l'excéution en temps réel, on peut choisir une valeur plus petite mais pas plus grande.  \n",
    "- **minimum_segment_size** : Ce paramètre permet de préciser la valeur minimale de noeuds qu'il faut pour que la conversion du réseau soit exécutée. En conséquence, en général on choisit des valeurs inférieures à 5. Ce paramètre permet également de choisir le nombre minimum de noeuds pendant l'optimisation finale INT8 et donc d'optimiser la précision des résultats finaux.  \n",
    "- **max_workspace_size_byte** : Les opérations d'optimisation de TensorRT ont besoin d'utiliser de l'espace de stockage temporaire. Ce paramètre permet de limiter l'espace maximal utilisé dans le GPU qu'une couche peut utiliser. Si une valeur insuffisante est donnée, TensorRT peut ne pas réussir à optimiser le modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83489ce-9a2f-42fe-b28c-d61452116238",
   "metadata": {},
   "source": [
    "Les arguments disponibles peuvent être obtenus à l'aide de la commande trtexec :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df4f99e-3db0-4f27-aa69-9f43a55829a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/src/tensorrt/bin/trtexec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8205c1-d6a3-4ea6-99d2-ffb271bc5a18",
   "metadata": {},
   "source": [
    "### Conversion au format FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20cb0dc-1e43-42cb-a928-44462cc83fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/src/tensorrt/bin/trtexec --verbose --fp16 --workspace=256 --maxBatch=1 --onnx=\"meilleur_model_colab/meilleur_model_colab.onnx\" --saveEngine=\"meilleur_model_colab/meilleur_modele_colab_FP16.engine\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
